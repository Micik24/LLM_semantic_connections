{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b49bc99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    results = {}\\n    for mask_index, token in queries:\\n        if mask_index >= len(mask_positions):\\n            raise ValueError(f\"Mask index {mask_index} out of range. Found {len(mask_positions)} masks.\")\\n\\n        pos = mask_positions[mask_index, 1]  # token position of that mask\\n        token_id = tokenizer.convert_tokens_to_ids(token)\\n        prob = probs[0, pos, token_id].item()\\n        results[(mask_index, token)] = prob\\n\\n    return results   '"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "\n",
    "# Load pretrained model\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "def masked_tokens_probabilities(sentence: str, M, word_prob, i):\n",
    "    \"\"\"\n",
    "    Calculate probabilities for multiple (mask_index, token) queries in one forward pass.\n",
    "\n",
    "    Parameters:\n",
    "    - sentence (str): Sentence containing one or more <mask> tokens.\n",
    "    - queries (list of tuples): Each tuple = (mask_index, token).\n",
    "        Example: [(0, \"Paris\"), (1, \"Berlin\")]\n",
    "\n",
    "    Returns:\n",
    "    - dict: { (mask_index, token): probability }\n",
    "    \"\"\"\n",
    "    # Tokenize once\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    # Find all mask positions\n",
    "    mask_positions = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=False)\n",
    "\n",
    "    # Forward pass once\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=-1)  # shape: [batch, seq_len, vocab_size]\n",
    "\n",
    "  #  print(mask_positions[:, 1][-1])\n",
    "\n",
    "    j = -1\n",
    "\n",
    "    with open(\"test_NGSL_2.csv\", newline=\"\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        for pos in mask_positions[:, 1]:\n",
    "            if pos > 2 and pos < (mask_positions[:, 1][-1] - 1):\n",
    "                for row in reader:\n",
    "                    j+=1\n",
    "                    for tok in row:\n",
    "                        token_id = tokenizer.convert_tokens_to_ids(tok)\n",
    "                        prob = probs[0, pos, token_id].item()\n",
    "                        M[i][j] += word_prob * prob\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "    results = {}\n",
    "    for mask_index, token in queries:\n",
    "        if mask_index >= len(mask_positions):\n",
    "            raise ValueError(f\"Mask index {mask_index} out of range. Found {len(mask_positions)} masks.\")\n",
    "\n",
    "        pos = mask_positions[mask_index, 1]  # token position of that mask\n",
    "        token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "        prob = probs[0, pos, token_id].item()\n",
    "        results[(mask_index, token)] = prob\n",
    "\n",
    "    return results   \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83b0a169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordfreq import word_frequency\n",
    "\n",
    "# Function to compute frequency of a word\n",
    "def get_word_frequency(word: str, lang: str = \"en\") -> float:\n",
    "    \"\"\"\n",
    "    Returns the frequency of a word in the given language.\n",
    "    By default, English (\"en\") is used.\n",
    "    \n",
    "    The result is a probability between 0 and 1.\n",
    "    \"\"\"\n",
    "    return word_frequency(word, lang)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c097f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "with open(\"test_NGSL.csv\", newline=\"\") as f:\n",
    "    n = sum(1 for _ in f)\n",
    "\n",
    "M = np.zeros((n, n), dtype=np.float64)\n",
    "\n",
    "i=-1\n",
    "\n",
    "with open(\"test_NGSL.csv\", newline=\"\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        i+=1\n",
    "        for word in row:\n",
    "            if(word != \"\"):\n",
    "                word_prob = get_word_frequency(word, \"en\")\n",
    "                masked_tokens_probabilities(\"<mask> <mask> <mask>\" + word + \"<mask> <mask> <mask> <mask> <mask> <mask>\", M, word_prob, i)\n",
    "            \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "deb93b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.12963541e-13, 5.64817705e-14, 1.12963541e-13],\n",
       "       [6.84060873e-13, 3.42030436e-13, 6.84060873e-13],\n",
       "       [6.09340342e-13, 3.04670171e-13, 6.09340342e-13]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63dcc4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roberta_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

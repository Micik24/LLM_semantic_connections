{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f29a13e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mask at position 1:\n",
      "  Word 'Ġshovel': logit=-0.65, prob=0.0000000413132319, rank=16789 / 50265\n",
      "  Top predictions:\n",
      "          </s> | logit=14.51 | prob=0.15902592\n",
      "\n",
      "Mask at position 2:\n",
      "  Word 'Ġshovel': logit=-0.43, prob=0.0000060175279941, rank=12895 / 50265\n",
      "  Top predictions:\n",
      "             A | logit=8.09 | prob=0.03012952\n",
      "\n",
      "Mask at position 3:\n",
      "  Word 'Ġshovel': logit=0.75, prob=0.0000060494016907, rank=8480 / 50265\n",
      "  Top predictions:\n",
      "    Ġconstruction | logit=9.73 | prob=0.04781074\n",
      "\n",
      "Mask at position 5:\n",
      "  Word 'Ġshovel': logit=2.55, prob=0.0000263756737695, rank=2415 / 50265\n",
      "  Top predictions:\n",
      "           Ġis | logit=10.60 | prob=0.08302768\n",
      "\n",
      "Mask at position 6:\n",
      "  Word 'Ġshovel': logit=1.10, prob=0.0000405749451602, rank=3031 / 50265\n",
      "  Top predictions:\n",
      "            Ġa | logit=8.21 | prob=0.04979895\n",
      "\n",
      "Mask at position 7:\n",
      "  Word 'Ġshovel': logit=1.11, prob=0.0000514188650413, rank=2461 / 50265\n",
      "  Top predictions:\n",
      "           Ġto | logit=7.64 | prob=0.03532176\n",
      "\n",
      "Mask at position 8:\n",
      "  Word 'Ġshovel': logit=0.50, prob=0.0000319341343129, rank=3875 / 50265\n",
      "  Top predictions:\n",
      "            Ġa | logit=7.55 | prob=0.03691186\n",
      "\n",
      "Mask at position 9:\n",
      "  Word 'Ġshovel': logit=0.58, prob=0.0000399403070332, rank=3850 / 50265\n",
      "  Top predictions:\n",
      "          Ġthe | logit=6.96 | prob=0.02354622\n",
      "\n",
      "Mask at position 10:\n",
      "  Word 'Ġshovel': logit=0.43, prob=0.0000155612942763, rank=6634 / 50265\n",
      "  Top predictions:\n",
      "             . | logit=9.99 | prob=0.22170848\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "\n",
    "def analyze_masks(sentence, model, tokenizer, search_word=None, top_n=5):\n",
    "    \"\"\"\n",
    "    For each <mask> token in the sentence:\n",
    "    - Show rank/logit/prob of a searched word (if provided).\n",
    "    - Show top_n most probable candidates.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits  # [batch, seq_len, vocab_size]\n",
    "\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    mask_positions = (inputs[\"input_ids\"] == mask_token_id).nonzero(as_tuple=True)[1]\n",
    "\n",
    "    search_word_id = None\n",
    "    if search_word:\n",
    "        search_word_id = tokenizer.convert_tokens_to_ids(search_word)\n",
    "\n",
    "    results = {}\n",
    "    for pos in mask_positions:\n",
    "        mask_logits = logits[0, pos]\n",
    "        probs = torch.softmax(mask_logits, dim=-1)\n",
    "\n",
    "        # Top-N predictions\n",
    "        top_logits, top_ids = torch.topk(mask_logits, top_n)\n",
    "        tokens = tokenizer.convert_ids_to_tokens(top_ids.tolist())\n",
    "        top_probs = probs[top_ids].tolist()\n",
    "\n",
    "        entry = {\n",
    "            \"top_predictions\": [\n",
    "                {\"token\": t, \"logit\": l.item(), \"prob\": p}\n",
    "                for t, l, p in zip(tokens, top_logits, top_probs)\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # If searching a specific word\n",
    "        if search_word_id is not None:\n",
    "            word_logit = mask_logits[search_word_id].item()\n",
    "            word_prob = probs[search_word_id].item()\n",
    "\n",
    "            sorted_ids = torch.argsort(mask_logits, descending=True)\n",
    "            rank = (sorted_ids == search_word_id).nonzero(as_tuple=True)[0].item() + 1  # 1-based\n",
    "\n",
    "            entry[\"searched_word\"] = {\n",
    "                \"word\": search_word,\n",
    "                \"logit\": word_logit,\n",
    "                \"prob\": word_prob,\n",
    "                \"rank\": rank,\n",
    "                \"vocab_size\": mask_logits.shape[0]\n",
    "            }\n",
    "\n",
    "        results[pos.item()] = entry\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"roberta-base\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "    sentence = \"<mask> <mask> <mask> worker <mask> <mask> <mask> <mask> <mask> <mask>\" \n",
    "\n",
    "    preds = analyze_masks(sentence, model, tokenizer, search_word=\"Ġshovel\", top_n=1)\n",
    "\n",
    "    for pos, info in preds.items():\n",
    "        if pos < 20:\n",
    "            print(f\"\\nMask at position {pos}:\")\n",
    "            if \"searched_word\" in info:\n",
    "                sw = info[\"searched_word\"]\n",
    "                print(f\"  Word '{sw['word']}': logit={sw['logit']:.2f}, prob={sw['prob']:.16f}, rank={sw['rank']} / {sw['vocab_size']}\")\n",
    "            print(\"  Top predictions:\")\n",
    "            for c in info[\"top_predictions\"]:\n",
    "                print(f\"    {c['token']:>10s} | logit={c['logit']:.2f} | prob={c['prob']:.8f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc565983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3907099308283998e-10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 0.0000001693114484 + 0.0000001858393830 + 0.0000005164816912 + 0.0000040519726099 + 0.0000061137752709\n",
    "z = y * 1.26e-05\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c88b64f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġdigging']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word = \" digging\"\n",
    "tokens = tokenizer.tokenize(word)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17989ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roberta_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f29a13e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask positions: [2]\n",
      "\n",
      "Mask  2 | logit(table)=6.75 | avg=-2.31 | rank(table)=354 / 50265\n",
      "   Top predictions:\n",
      "      ĠSahara         (logit=16.91)\n",
      "      Ġdesert         (logit=14.36)\n",
      "      ĠLevant         (logit=12.65)\n",
      "      ĠMoon           (logit=12.41)\n",
      "      ĠAmazon         (logit=12.04)\n",
      "      ĠSinai          (logit=11.97)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "\n",
    "# Load RoBERTa\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Define the sentence (with masks)\n",
    "sentence = \"The <mask> is the largest desert in the world.\"\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits  # [batch, seq_len, vocab_size]\n",
    "\n",
    "# Get mask positions\n",
    "mask_token_id = tokenizer.mask_token_id\n",
    "mask_positions = (inputs[\"input_ids\"] == mask_token_id).nonzero(as_tuple=True)[1]\n",
    "print(\"Mask positions:\", mask_positions.tolist())\n",
    "\n",
    "# Get word ID\n",
    "word = \"ĠAntarctica\"\n",
    "word_id = tokenizer.convert_tokens_to_ids(word)\n",
    "\n",
    "# Number of alternatives to display\n",
    "top_n = 6\n",
    "\n",
    "# Iterate over all masks\n",
    "results = []\n",
    "for pos in mask_positions:\n",
    "    mask_logits = logits[0, pos]\n",
    "\n",
    "    # Word logit\n",
    "    word_logit = mask_logits[word_id].item()\n",
    "\n",
    "    # Rank of the word\n",
    "    sorted_scores, sorted_ids = torch.sort(mask_logits, descending=True)\n",
    "    rank = (sorted_ids == word_id).nonzero(as_tuple=True)[0].item() + 1  # 1-based\n",
    "\n",
    "    # Average logit for context\n",
    "    avg_logit = mask_logits.mean().item()\n",
    "\n",
    "    # Top N predictions\n",
    "    top_scores, top_ids = torch.topk(mask_logits, top_n)\n",
    "    top_tokens = tokenizer.convert_ids_to_tokens(top_ids.tolist())\n",
    "    top_predictions = [(tok, float(score)) for tok, score in zip(top_tokens, top_scores)]\n",
    "\n",
    "    results.append({\n",
    "        \"mask_index\": pos.item(),\n",
    "        \"word_logit\": word_logit,\n",
    "        \"avg_logit\": avg_logit,\n",
    "        \"rank\": rank,\n",
    "        \"vocab_size\": mask_logits.shape[0],\n",
    "        \"top_predictions\": top_predictions\n",
    "    })\n",
    "\n",
    "# Print nicely\n",
    "for r in results:\n",
    "    print(\n",
    "        f\"\\nMask {r['mask_index']:2d} | \"\n",
    "        f\"logit(table)={r['word_logit']:.2f} | \"\n",
    "        f\"avg={r['avg_logit']:.2f} | \"\n",
    "        f\"rank(table)={r['rank']} / {r['vocab_size']}\"\n",
    "    )\n",
    "    print(\"   Top predictions:\")\n",
    "    for tok, score in r[\"top_predictions\"]:\n",
    "        print(f\"      {tok:15s} (logit={score:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c88b64f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ĠAntarctica']\n"
     ]
    }
   ],
   "source": [
    "word = \" Antarctica\"\n",
    "tokens = tokenizer.tokenize(word)\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roberta_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

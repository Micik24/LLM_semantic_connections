{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f29a13e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mask at position 6:\n",
      "  Word 'Ġpages': logit=0.94, prob=0.0000, rank=5210 / 50265\n",
      "  Top predictions:\n",
      "          Ġnew | logit=10.81 | prob=0.1607\n",
      "          Ġthe | logit=10.62 | prob=0.1332\n",
      "            's | logit=9.74 | prob=0.0551\n",
      "            Ġa | logit=9.19 | prob=0.0318\n",
      "          ĠThe | logit=9.15 | prob=0.0305\n",
      "        Ġfirst | logit=9.12 | prob=0.0297\n",
      "          Ġhis | logit=8.82 | prob=0.0219\n",
      "         Ġthis | logit=8.73 | prob=0.0201\n",
      "       Ġsecond | logit=8.62 | prob=0.0180\n",
      "    Ġbestselling | logit=8.35 | prob=0.0138\n",
      "       Ġlatest | logit=8.24 | prob=0.0122\n",
      "             , | logit=8.04 | prob=0.0101\n",
      "         Ġnext | logit=7.77 | prob=0.0077\n",
      "           The | logit=7.68 | prob=0.0070\n",
      "         ĠThis | logit=7.63 | prob=0.0067\n",
      "        Ġthird | logit=7.46 | prob=0.0056\n",
      "          Ġher | logit=7.43 | prob=0.0055\n",
      "         Ġbest | logit=7.39 | prob=0.0052\n",
      "           Ġof | logit=7.29 | prob=0.0048\n",
      "    Ġexcellent | logit=7.16 | prob=0.0042\n",
      "            ĠA | logit=7.10 | prob=0.0039\n",
      "           Ġmy | logit=7.01 | prob=0.0036\n",
      "           Ġto | logit=6.99 | prob=0.0035\n",
      "       Ġnewest | logit=6.96 | prob=0.0034\n",
      "             ' | logit=6.90 | prob=0.0032\n",
      "          Ġand | logit=6.89 | prob=0.0032\n",
      "      Ġanother | logit=6.88 | prob=0.0032\n",
      "       Ġfourth | logit=6.85 | prob=0.0031\n",
      "         Ġlast | logit=6.83 | prob=0.0030\n",
      "           Ġin | logit=6.80 | prob=0.0029\n",
      "         Ġfull | logit=6.69 | prob=0.0026\n",
      "        Ġother | logit=6.67 | prob=0.0026\n",
      "        Ġdebut | logit=6.63 | prob=0.0025\n",
      "             s | logit=6.62 | prob=0.0024\n",
      "         Ġgood | logit=6.62 | prob=0.0024\n",
      "        Ġfinal | logit=6.55 | prob=0.0023\n",
      "        Ġcomic | logit=6.54 | prob=0.0022\n",
      "        Ġgreat | logit=6.51 | prob=0.0022\n",
      "           new | logit=6.49 | prob=0.0021\n",
      "          ĠHis | logit=6.48 | prob=0.0021\n",
      "        Ġaudio | logit=6.45 | prob=0.0020\n",
      "           Ġno | logit=6.43 | prob=0.0020\n",
      "     Ġupcoming | logit=6.41 | prob=0.0020\n",
      "      Ġgraphic | logit=6.41 | prob=0.0020\n",
      "     Ġcomplete | logit=6.40 | prob=0.0020\n",
      "           Ġby | logit=6.39 | prob=0.0019\n",
      "          Ġown | logit=6.39 | prob=0.0019\n",
      "         Ġthat | logit=6.38 | prob=0.0019\n",
      "    Ġforthcoming | logit=6.30 | prob=0.0018\n",
      "         Ġyour | logit=6.28 | prob=0.0017\n",
      "     Ġfavorite | logit=6.28 | prob=0.0017\n",
      "            th | logit=6.27 | prob=0.0017\n",
      "       Ġrecent | logit=6.27 | prob=0.0017\n",
      "            Ġ- | logit=6.25 | prob=0.0017\n",
      "      Ġpicture | logit=6.22 | prob=0.0016\n",
      "          Ġone | logit=6.14 | prob=0.0015\n",
      "           the | logit=6.12 | prob=0.0015\n",
      "          ĠNew | logit=6.11 | prob=0.0015\n",
      "             ) | logit=6.10 | prob=0.0015\n",
      "          This | logit=6.10 | prob=0.0014\n",
      "       Ġentire | logit=6.09 | prob=0.0014\n",
      "           Ġis | logit=6.08 | prob=0.0014\n",
      "        Ġshort | logit=6.08 | prob=0.0014\n",
      "           Ġon | logit=6.06 | prob=0.0014\n",
      "           New | logit=6.05 | prob=0.0014\n",
      "        Ġfifth | logit=6.01 | prob=0.0013\n",
      "         Ġ2017 | logit=6.01 | prob=0.0013\n",
      "             : | logit=5.95 | prob=0.0012\n",
      "       Ġreview | logit=5.94 | prob=0.0012\n",
      "    Ġwonderful | logit=5.92 | prob=0.0012\n",
      "             A | logit=5.90 | prob=0.0012\n",
      "       Ġsuperb | logit=5.89 | prob=0.0012\n",
      "         Ġfrom | logit=5.85 | prob=0.0011\n",
      "          Ġfor | logit=5.84 | prob=0.0011\n",
      "           Ġas | logit=5.81 | prob=0.0011\n",
      "     Ġoriginal | logit=5.80 | prob=0.0011\n",
      "         Ġrare | logit=5.78 | prob=0.0011\n",
      "      Ġfantasy | logit=5.77 | prob=0.0010\n",
      "          ĠOne | logit=5.77 | prob=0.0010\n",
      "      ĠEnglish | logit=5.76 | prob=0.0010\n",
      "        Ġsixth | logit=5.74 | prob=0.0010\n",
      "         Ġmore | logit=5.73 | prob=0.0010\n",
      "       fiction | logit=5.72 | prob=0.0010\n",
      "    Ġcompanion | logit=5.69 | prob=0.0010\n",
      "         cover | logit=5.69 | prob=0.0010\n",
      "    Ġpublished | logit=5.69 | prob=0.0010\n",
      "      Ġmystery | logit=5.67 | prob=0.0009\n",
      "         Ġ2013 | logit=5.67 | prob=0.0009\n",
      "           ĠMy | logit=5.66 | prob=0.0009\n",
      "         Ġ2014 | logit=5.64 | prob=0.0009\n",
      "      Ġseventh | logit=5.61 | prob=0.0009\n",
      "          Ġart | logit=5.56 | prob=0.0008\n",
      "        Ġguest | logit=5.56 | prob=0.0008\n",
      "             a | logit=5.53 | prob=0.0008\n",
      "      ĠAnother | logit=5.51 | prob=0.0008\n",
      "        ĠFirst | logit=5.50 | prob=0.0008\n",
      "      Ġfiction | logit=5.49 | prob=0.0008\n",
      "        Ġtheir | logit=5.48 | prob=0.0008\n",
      "         Ġ2016 | logit=5.46 | prob=0.0008\n",
      "    Ġbrilliant | logit=5.46 | prob=0.0008\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "\n",
    "def analyze_masks(sentence, model, tokenizer, search_word=None, top_n=5):\n",
    "    \"\"\"\n",
    "    For each <mask> token in the sentence:\n",
    "    - Show rank/logit/prob of a searched word (if provided).\n",
    "    - Show top_n most probable candidates.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits  # [batch, seq_len, vocab_size]\n",
    "\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    mask_positions = (inputs[\"input_ids\"] == mask_token_id).nonzero(as_tuple=True)[1]\n",
    "\n",
    "    search_word_id = None\n",
    "    if search_word:\n",
    "        search_word_id = tokenizer.convert_tokens_to_ids(search_word)\n",
    "\n",
    "    results = {}\n",
    "    for pos in mask_positions:\n",
    "        mask_logits = logits[0, pos]\n",
    "        probs = torch.softmax(mask_logits, dim=-1)\n",
    "\n",
    "        # Top-N predictions\n",
    "        top_logits, top_ids = torch.topk(mask_logits, top_n)\n",
    "        tokens = tokenizer.convert_ids_to_tokens(top_ids.tolist())\n",
    "        top_probs = probs[top_ids].tolist()\n",
    "\n",
    "        entry = {\n",
    "            \"top_predictions\": [\n",
    "                {\"token\": t, \"logit\": l.item(), \"prob\": p}\n",
    "                for t, l, p in zip(tokens, top_logits, top_probs)\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # If searching a specific word\n",
    "        if search_word_id is not None:\n",
    "            word_logit = mask_logits[search_word_id].item()\n",
    "            word_prob = probs[search_word_id].item()\n",
    "\n",
    "            sorted_ids = torch.argsort(mask_logits, descending=True)\n",
    "            rank = (sorted_ids == search_word_id).nonzero(as_tuple=True)[0].item() + 1  # 1-based\n",
    "\n",
    "            entry[\"searched_word\"] = {\n",
    "                \"word\": search_word,\n",
    "                \"logit\": word_logit,\n",
    "                \"prob\": word_prob,\n",
    "                \"rank\": rank,\n",
    "                \"vocab_size\": mask_logits.shape[0]\n",
    "            }\n",
    "\n",
    "        results[pos.item()] = entry\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"roberta-base\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "    sentence = \"<mask> <mask> <mask> <mask> <mask> <mask> book <mask> <mask> <mask> <mask> <mask> <mask> <mask>\"\n",
    "\n",
    "    preds = analyze_masks(sentence, model, tokenizer, search_word=\"Ġpages\", top_n=100)\n",
    "\n",
    "    for pos, info in preds.items():\n",
    "        if pos == 6:\n",
    "            print(f\"\\nMask at position {pos}:\")\n",
    "            if \"searched_word\" in info:\n",
    "                sw = info[\"searched_word\"]\n",
    "                print(f\"  Word '{sw['word']}': logit={sw['logit']:.2f}, prob={sw['prob']:.4f}, rank={sw['rank']} / {sw['vocab_size']}\")\n",
    "            print(\"  Top predictions:\")\n",
    "            for c in info[\"top_predictions\"]:\n",
    "                print(f\"    {c['token']:>10s} | logit={c['logit']:.2f} | prob={c['prob']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c88b64f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to', 'ilet']\n"
     ]
    }
   ],
   "source": [
    "word = \"toilet\"\n",
    "tokens = tokenizer.tokenize(word)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17989ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roberta_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

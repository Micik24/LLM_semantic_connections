{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2878545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# Choose RoBERTa (good balance of quality & resource use)\n",
    "model_name = \"roberta-base\"\n",
    "\n",
    "# Load tokenizer (converts words to IDs, handles <mask>)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load masked language model (predicts missing tokens)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f732692",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"<mask> apple <mask> <mask> <mask> <mask> <mask>.\"\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18cbff94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():  # no gradients needed (faster, less memory)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits  # shape: [batch, seq_len, vocab_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d615906e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask positions: tensor([1, 3, 4, 5, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "mask_token_id = tokenizer.mask_token_id\n",
    "mask_positions = (inputs[\"input_ids\"] == mask_token_id).nonzero(as_tuple=True)[1]\n",
    "print(\"Mask positions:\", mask_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a01cf347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit for 'round': 5.32\n",
      "Average logit across vocab: -1.72\n",
      "Above average? True\n"
     ]
    }
   ],
   "source": [
    "# Pick the 5th mask (index 4)\n",
    "mask_index4 = mask_positions[0]\n",
    "\n",
    "# Get logits for that mask\n",
    "mask_logits = logits[0, mask_index4]\n",
    "\n",
    "# Look up the token ID for \"round\"\n",
    "round_id = tokenizer.convert_tokens_to_ids(\"round\")\n",
    "round_logit = mask_logits[round_id].item()\n",
    "\n",
    "# Compare with average logit\n",
    "avg_logit = mask_logits.mean().item()\n",
    "\n",
    "print(f\"Logit for 'round': {round_logit:.2f}\")\n",
    "print(f\"Average logit across vocab: {avg_logit:.2f}\")\n",
    "print(\"Above average?\", round_logit > avg_logit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e36cdb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An              16.85\n",
      "The             16.85\n",
      "This            14.62\n",
      "One             13.99\n",
      "My              13.86\n",
      "an              13.81\n",
      "Another         13.57\n",
      "Every           13.21\n",
      "the             13.17\n",
      "How             13.10\n",
      "Two             13.09\n",
      "No              12.98\n",
      "That            12.81\n",
      "Apple           12.80\n",
      "Fresh           12.64\n",
      "American        12.59\n",
      "Three           12.51\n",
      "And             12.49\n",
      "Green           12.47\n",
      "For             12.41\n",
      "More            12.40\n",
      "Each            12.35\n",
      "First           12.31\n",
      "1               12.25\n",
      "These           12.22\n",
      "With            12.22\n",
      "Some            12.22\n",
      "Traditional     12.16\n",
      "On              12.10\n",
      "Our             12.09\n",
      "As              12.00\n",
      "Rare            11.98\n",
      "What            11.93\n",
      "Not             11.90\n",
      "Your            11.88\n",
      "Big             11.87\n",
      "Raw             11.85\n",
      "Classic         11.85\n",
      "A               11.84\n",
      "New             11.80\n",
      "Red             11.78\n",
      "and             11.77\n",
      "Four            11.74\n",
      "Next            11.72\n",
      "When            11.70\n",
      "Most            11.67\n",
      "Making          11.66\n",
      "Little          11.63\n",
      "3               11.62\n",
      "Why             11.59\n",
      "In              11.56\n",
      "Perfect         11.54\n",
      "Blue            11.54\n",
      "From            11.53\n",
      "Best            11.50\n",
      "Fake            11.43\n",
      "2               11.40\n",
      "Like            11.39\n",
      "Free            11.38\n",
      "Pink            11.33\n",
      "Golden          11.30\n",
      "Or              11.29\n",
      "Chinese         11.28\n",
      "Baby            11.28\n",
      "Bad             11.27\n",
      "Small           11.26\n",
      "Both            11.20\n",
      "Sweet           11.17\n",
      "one             11.17\n",
      "Common          11.14\n",
      "Simple          11.11\n",
      "Using           11.11\n",
      "Black           11.11\n",
      "Her             11.10\n",
      "Happy           11.08\n",
      "Large           11.06\n",
      "Wild            11.06\n",
      "Left            11.05\n",
      "His             11.04\n",
      "See             11.01\n",
      "At              10.99\n",
      "Old             10.99\n",
      "Neither         10.96\n",
      "I               10.96\n",
      "two             10.91\n",
      "Pure            10.89\n",
      "Which           10.89\n",
      "Yellow          10.89\n",
      "Insert          10.88\n",
      "White           10.87\n",
      "4               10.82\n",
      "Natural         10.79\n",
      "Those           10.76\n",
      "Japanese        10.76\n",
      "After           10.76\n",
      "Half            10.76\n",
      "Single          10.73\n",
      "Used            10.71\n",
      "Individual      10.70\n",
      "Double          10.70\n"
     ]
    }
   ],
   "source": [
    "topk = torch.topk(mask_logits, 100)\n",
    "#test\n",
    "for idx, score in zip(topk.indices, topk.values):\n",
    "    print(f\"{tokenizer.decode([idx]):<15} {score.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0408919e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'round' has logit 5.32\n",
      "Rank: 3465 out of 50265\n"
     ]
    }
   ],
   "source": [
    "word = \"round\"\n",
    "word_id = tokenizer.convert_tokens_to_ids(word)\n",
    "\n",
    "# Get logit score for that word\n",
    "word_logit = mask_logits[word_id].item()\n",
    "\n",
    "# Sort all logits (descending: highest first)\n",
    "sorted_scores, sorted_ids = torch.sort(mask_logits, descending=True)\n",
    "\n",
    "# Find rank of the word\n",
    "rank = (sorted_ids == word_id).nonzero(as_tuple=True)[0].item() + 1  # +1 for 1-based rank\n",
    "vocab_size = mask_logits.shape[0]\n",
    "\n",
    "print(f\"Word '{word}' has logit {word_logit:.2f}\")\n",
    "print(f\"Rank: {rank} out of {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2d1389",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"Trump\"\n",
    "tokens = tokenizer.tokenize(word)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1651a821",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roberta_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
